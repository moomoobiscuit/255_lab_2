---
title: "ENGSCI 255 Lab 2"
author: "Navindd Raj"
date: "Due Date: 5pm Monday 28 May 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("ggplot2")
library("rpart")
library("rpart.plot")
library("rattle")
library("gridExtra")
library("randomForest")
theme_update(plot.title = element_text(hjust = 0.5))
theme_update(plot.subtitle = element_text(hjust = 0.5))
```

##Question 1

```{r}
set.seed(99)  # sets a seed for random number generator
```

####a) k-means clustering using all independent attributes:

```{r}
spam.df = read.csv("spamdata.csv",header=TRUE) # read in the data

# perform a k-means clustering with 20 iterations, using all 57 attributes
spamCluster = kmeans(spam.df, 2, nstart = 20, iter.max = 20)
spamCluster$size    # size of each cluster
```

Before we check the results of the clustering, we should find out how many emails are actually spam.
```{r}
numOfSpam = 0
for (i in seq(1,length(spam.df$IsSpam))) {
     if (spam.df$IsSpam[i] == 1) {
         numOfSpam = numOfSpam + 1
     }
}
print(numOfSpam)
```

We can see there are two clusters of size 4357 and 244, but there are 1813 spam emails in the dataset. So there will be a lot of false predictions in our clustered data. 

####b) Generate a table for the clustering:

Next, let's generate a table for the clustering results:
```{r}
# columns are is/is not spam, rows are the two clusters created
table(spamCluster$cluster, spam.df$IsSpam, dnn = c("Cluster No", "Is/Is not Spam"))
```

It seems like cluster 1 is predicting the group that isn't spam, and cluster 2 is predicting the group that is spam. 

####c) Calculations:

We can make the attribute each cluster is predicting the name of the cluster, so it is easier to understand the calculations. We say that something being spam is the positive result (1 or yes). 
```{r}
for (i in seq(1,length(spamCluster$cluster))) {
     # reduce the value by 1 so that each cluster represents what it is predicting
     spamCluster$cluster[i] = spamCluster$cluster[i] - 1
}

table(spamCluster$cluster, spam.df$IsSpam, dnn = c("Prediction", "Actual"))
```

If we used this clustering as a foundation for a spam filter, what is the accuracy, sensitivity, specificity and precision of the filter? 

Number of true negatives (TN) = 2735. Number of true positives (TP) = 191. Number of false positives (FP) = 53. Number of false negatives (FN) = 1622. 

Accuracy = proportion of datapoints classified correctly = (TN+TP)/(total # of datapoints) = (2735+191)/(2735+191+53+1622) = 0.6359, or 63.59%

Sensitivity = true positive rate = TP/(TP+FN) = 191/(191+1622) = 0.1054, or 10.54%. This is the proportion of datapoints which are predicted to be yes, and are actually yes. 

Specificity = true negative rate = TN/(TN+FP) = 2735/(2735+53) = 0.9810, or 98.10%. This is the proportion of datapoints which are predicted to be no, and are actually no. 

Precision = TP/(TP+FP) = 191/(191+53) = 0.7828, or 78.28%. This is the proportion of datapoints which are actually yes, when they are predicted to be yes. 

##Question 2

```{r}
set.seed(50)  # sets a seed for random number generator
```

Generate a training dataset of 2400 datapoints, where the remaining points will be the test set: 
```{r}
# training subset of 2400 datapoints, half is random spam emails, half is random not spam emails
# dataset is ordered so we know we are getting a 50/50 mix of 
train = c(sample(1:numOfSpam,1200),sample(numOfSpam+1:length(spam.df$IsSpam),1200))

# treat spam classification as a factor instead of numeric
spam.df$IsSpam = as.factor(spam.df$IsSpam)
```

####a) Loss Matrices:

#####**i.** 

Set the termination criteria of max depth of 10. Disable any other termination criteria:
```{r, fig.align='center'}
# cp to 0 as we want to branch any time possible, minimum complexity parameter, minsplit 1
controlParms = rpart.control(maxdepth = 10, cp = 0, minsplit = 1)
```

#####**ii.**

We want to modify the loss matrix to generate 6 different trees using all independent attributes, ranging from no FP's to no FN's in the training data. This means we should put a heavy cost on the false positives and low cost on false negatives, and then constantly make FP's lower cost and FN's higher cost. The different elements in the matrix correspond to the costs of the following: 
```{r}
list(loss = matrix(c("TN","FN","FP","TP"), nrow = 2))
```

**Tree 1** (aiming for no false positives), cost of 95 for FP's, 5 for FN's:
```{r}
# 95/5 cost for FP/FN
lossMatrix1 = list(loss = matrix(c(0,95,5,0), nrow = 2))

# create the tree using the preset control and the loss matrix
tree1 = rpart(IsSpam~., data = spam.df, subset = train, control = controlParms, parms = lossMatrix1)

# in-sample prediction using the training data:
ISCM1 = table(predict(tree1,spam.df[train,],type="class"),spam.df[train,"IsSpam"], dnn = c("Prediction", "Actual"))
ISCM1
```

**Tree 2** (still a high weight on false positives), cost of 70 for FP's, 30 for FN's:
```{r}
# 70/30 cost for FP/FN
lossMatrix2 = list(loss = matrix(c(0,70,30,0), nrow = 2))

# create the tree using the preset control and the loss matrix
tree2 = rpart(IsSpam~., data = spam.df, subset = train, control = controlParms, parms = lossMatrix2)

# in-sample prediction using the training data:
ISCM2 = table(predict(tree2,spam.df[train,],type="class"),spam.df[train,"IsSpam"], dnn = c("Prediction", "Actual"))
ISCM2
```

**Tree 3** (slightly higher weight on false negatives vs false positives), cost of 45 for FP's, 65 for FN's:
```{r}
# 45/65 cost for FP/FN
lossMatrix3 = list(loss = matrix(c(0,45,65,0), nrow = 2))

# create the tree using the preset control and the loss matrix
tree3 = rpart(IsSpam~., data = spam.df, subset = train, control = controlParms, parms = lossMatrix3)

# in-sample prediction using the training data:
ISCM3 = table(predict(tree3,spam.df[train,],type="class"),spam.df[train,"IsSpam"], dnn = c("Prediction", "Actual"))
ISCM3
```

**Tree 4** (even higher weighting on false negatives), cost of 20 for FP's, 80 for FN's:
```{r}
# 20,80 cost for FP/FN
lossMatrix4 = list(loss = matrix(c(0,20,80,0), nrow = 2))

# create the tree using the preset control and the loss matrix
tree4 = rpart(IsSpam~., data = spam.df, subset = train, control = controlParms, parms = lossMatrix4)

# in-sample prediction using the training data:
ISCM4 = table(predict(tree4,spam.df[train,],type="class"),spam.df[train,"IsSpam"], dnn = c("Prediction", "Actual"))
ISCM4
```

**Tree 5** (aiming for no false negatives), cost of 5 for FP's, 95 for FN's:
```{r}
# 5/95 cost for FP/FN
lossMatrix5 = list(loss = matrix(c(0,5,95,0), nrow = 2))

# create the tree using the preset control and the loss matrix
tree5 = rpart(IsSpam~., data = spam.df, subset = train, control = controlParms, parms = lossMatrix5)

# in-sample prediction using the training data:
ISCM5 = table(predict(tree5,spam.df[train,],type="class"),spam.df[train,"IsSpam"], dnn = c("Prediction", "Actual"))
ISCM5
```

**Tree 6** (let's see what happens if we put an even higher weight now that we have achieved no false negatives already), cost of 1 for FP's, 99 for FN's:
```{r}
# 1/99 cost for FP/FN
lossMatrix6 = list(loss = matrix(c(0,1,99,0), nrow = 2))

# create the tree using the preset control and the loss matrix
tree6 = rpart(IsSpam~., data = spam.df, subset = train, control = controlParms, parms = lossMatrix6)

# in-sample prediction using the training data:
ISCM6 = table(predict(tree6,spam.df[train,],type="class"),spam.df[train,"IsSpam"], dnn = c("Prediction", "Actual"))
ISCM6
```

#####**iii.** 

The in-sample confusion matrices have already been shown in part ii above, so I will just show the confusion matrices for the out-of-sample predictions, using the test set (original dataset minus the training set).

**Tree 1:**
```{r}
# out-of-sample prediction using the test data:
OOSCM1 = table(predict(tree1,spam.df[-train,],type="class"),spam.df[-train,"IsSpam"], dnn = c("Prediction", "Actual"))
OOSCM1
```

**Tree 2:**
```{r}
# out-of-sample prediction using the test data:
OOSCM2 = table(predict(tree2,spam.df[-train,],type="class"),spam.df[-train,"IsSpam"], dnn = c("Prediction", "Actual"))
OOSCM2
```

**Tree 3:**
```{r}
# out-of-sample prediction using the test data:
OOSCM3 = table(predict(tree3,spam.df[-train,],type="class"),spam.df[-train,"IsSpam"], dnn = c("Prediction", "Actual"))
OOSCM3
```

**Tree 4:**
```{r}
# out-of-sample prediction using the test data:
OOSCM4 = table(predict(tree4,spam.df[-train,],type="class"),spam.df[-train,"IsSpam"], dnn = c("Prediction", "Actual"))
OOSCM4
```

**Tree 5:**
```{r}
# out-of-sample prediction using the test data:
OOSCM5 = table(predict(tree5,spam.df[-train,],type="class"),spam.df[-train,"IsSpam"], dnn = c("Prediction", "Actual"))
OOSCM5
```

**Tree 6:**
```{r}
# out-of-sample prediction using the test data:
OOSCM6 = table(predict(tree6,spam.df[-train,],type="class"),spam.df[-train,"IsSpam"], dnn = c("Prediction", "Actual"))
OOSCM6
```

#####**iv.** 

Put the values of sensitivity and specificity for each classification model into a vector to then show on a scatterplot (including both in and out-of-sample performance): [Code for sensitivity/specificity calculations hidden because messy]

```{r, echo = FALSE}
inSampleSensitivity = c(ISCM1[4]/(ISCM1[4]+ISCM1[3]),ISCM2[4]/(ISCM2[4]+ISCM2[3]),ISCM3[4]/(ISCM3[4]+ISCM3[3]),ISCM4[4]/(ISCM4[4]+ISCM4[3]),ISCM5[4]/(ISCM5[4]+ISCM5[3]),ISCM6[4]/(ISCM6[4]+ISCM6[3]))

inSampleSpecificity = c(ISCM1[1]/(ISCM1[1]+ISCM1[2]),ISCM2[1]/(ISCM2[1]+ISCM2[2]),ISCM3[1]/(ISCM3[1]+ISCM3[2]),ISCM4[1]/(ISCM4[1]+ISCM4[2]),ISCM5[1]/(ISCM5[1]+ISCM5[2]),ISCM6[1]/(ISCM6[1]+ISCM6[2]))

outOfSampleSensitivity = c(OOSCM1[4]/(OOSCM1[4]+OOSCM1[3]),OOSCM2[4]/(OOSCM2[4]+OOSCM2[3]),OOSCM3[4]/(OOSCM3[4]+OOSCM3[3]),OOSCM4[4]/(OOSCM4[4]+OOSCM4[3]),OOSCM5[4]/(OOSCM5[4]+OOSCM5[3]),OOSCM6[4]/(OOSCM6[4]+OOSCM6[3]))

outOfSampleSpecificity = c(OOSCM1[1]/(OOSCM1[1]+OOSCM1[2]),OOSCM2[1]/(OOSCM2[1]+OOSCM2[2]),OOSCM3[1]/(OOSCM3[1]+OOSCM3[2]),OOSCM4[1]/(OOSCM4[1]+OOSCM4[2]),OOSCM5[1]/(OOSCM5[1]+OOSCM5[2]),OOSCM6[1]/(OOSCM6[1]+OOSCM6[2]))

performance1.df = data.frame(inSampleSensitivity, inSampleSpecificity)
performance2.df = data.frame(outOfSampleSensitivity, outOfSampleSpecificity)
```

Show this information on a 2D scatterplot of sensitivity vs specificity, again, the in-sample performance was using the training dataset and out-of-sample performance used the original dataset minus the training set. Note the axes don't start from 0: 
```{r, fig.align = 'center',fig.height=5}
plot(inSampleSpecificity ~ inSampleSensitivity, data = performance1.df, col = "orange", pch = 16, cex = 1.2, ylim = c(0.6,1), xlim = c(0.7,1), ann = FALSE)
points(outOfSampleSpecificity ~ outOfSampleSensitivity, data = performance2.df, col = "blue", pch = 4, cex = 1.4)
legend("bottomleft", legend = c("In-sample performance", "Out-of-sample performance"), pch = c(16,4), col = c("orange","blue"))
title(main = "Specificity vs Sensitivity for a set of classification trees\nwith varying loss matrices", xlab = "Sensitivity", ylab = "Specificity")
```

####b) Tree Depth:

#####**i.** 

We want to generate six classification trees that vary by their max depth (from 5 to 30). For this I have disabled any other stopping criteria as otherwise the higher depth trees will all look the same and not go until their max depth has been reached.

To save space, I used a list to store all 6 trees generated, indexed 1 through 6 corresponding to max depth of 5 through 30 in steps of 5 for each tree:
```{r}
# create a list to store the trees
trees = list()

for (i in seq(1,6)) {
  # max depth of 5 to 30, disable other stopping criteria
  controlParms = rpart.control(maxdepth = 5*i, cp = 0, minsplit = 1) 
  
  # create a tree for each max depth
  trees[[i]] = rpart(IsSpam~.,data = spam.df, subset = train, control = controlParms)
}
```

#####**ii.**

Plot the tree for a depth of 5:
```{r, fig.align = 'center', fig.height = 5}
fancyRpartPlot(trees[[1]], sub = "Tree using all independent vars, max depth 5")
```

#####**iii.**

We now want to plot the *accuracies* of the classification models vs the max depths, one line for in-sample performance and one line for out-of-sample performance.

First of all, let's get some vectors of the accuracies for each tree depth and their predictions for both in and out-of-sample data:
```{r}
# create vectors to hold accuracies
ISAcc = c()
OOSAcc = c()

# generate confusion matrices, then put accuracies in lists for their respective predictions
for (i in seq(1,6)) {
  ISCM = table(predict(trees[[i]],spam.df[train,],type="class"),spam.df[train,"IsSpam"])
  OOSCM = table(predict(trees[[i]],spam.df[-train,],type="class"),spam.df[-train,"IsSpam"])
  
  # I couldn't simply use length(train) and length(spam.df$IsSpam - train) because the number of
  # predictions generated did not match the number of datapoints in the training set/ test set...
  # idk why, but the number of datapoints in the train+test results does sum to total # of datapoints
  ISAcc[i] = (ISCM[1]+ISCM[4])/(ISCM[1]+ISCM[2]+ISCM[3]+ISCM[4])
  OOSAcc[i] = (OOSCM[1]+OOSCM[4])/(OOSCM[1]+OOSCM[2]+OOSCM[3]+OOSCM[4])
}
```

Now let's try plot the accuracies against the max depths for each tree:
```{r, fig.align = 'center'}
# create a vector holding the max depths
maxDepths = seq(5,30,5)

# plot the data and add information about it
plot(ISAcc ~ maxDepths, type = "n", ylim = c(0.85,1), ann = FALSE)
lines(ISAcc ~ maxDepths, col = "orange")
lines(OOSAcc ~ maxDepths, col = "blue")
legend("bottomleft", legend = c("In-sample performance", "Out-of-sample performance"), pch = "_", col = c("orange","blue"))
title(main = "Accuracies for a set of trees with varying max depths\n(with other stopping criteria disabled)", xlab = "Max Depth", ylab = "Accuracy")
```

####c) Discuss the results above:

From the scatterplot (where max depth was constant and we changed the loss matrices for each tree), we can infer that in general, as you try to increase the sensitivity of a classification tree, the specificity will drop as a result, and vice versa for increasing specificity. For all trees, we have a decent ratio of specificity to sensitivity, meaning we have hit reasonably optimal solutions, assuming we are weighting specificity and sensitivity the same in a bi-objective optimasation problem. 

From the line graph (where the loss matrices were kept constant and we changed the max depth for each tree), we can see that there are two different trends. The in-sample performance kept improving as we increased the max depth of the tree, but the out-of-sample performance started dropping after the max depth of the tree was higher than 10. This is probably due to overfitting the classification model to the training data, so when applied to unseen data, it performs poorly as it has been conditioned too far with the training data. So we can see that the peak optimality (if we are aiming for accuracy) in this case is at a max tree depth of 10, which is what we were using for when we were varying the loss matrices in the first part of the question. 

In both experiments, we can see that the out-of-sample performance was poorer than the in-sample performance, which is expected. We should also consider the fact that we may have overfit the trees for both experiments, and to see if there is a more optimal max depth to use, we should test further by checking varying max depths between 5 and 15 in smaller increments, and possible change the stopping criteria so that it does have a minimum split of higher than 1 and requires some threshold change in complexity before branching out. 









